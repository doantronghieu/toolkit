# My code starts from here
# Standard library imports
import copy
import io
import time
import yaml
from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, Tuple, List, Callable, TypedDict, Union

# Third-party imports
import hydra
from loguru import logger
from omegaconf import DictConfig
import matplotlib.pyplot as plt
import numpy as np

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.prune as prune
import torch.ao.quantization
import torch.ao.quantization.quantize_fx as quantize_fx
from torch.ao.quantization import (
    QConfigMapping, float16_dynamic_qconfig, default_per_channel_qconfig,
    default_qconfig, QConfig, get_default_qconfig, get_default_qat_qconfig,
    get_default_qconfig_mapping, propagate_qconfig_, default_dynamic_qconfig,
    prepare_qat, prepare, convert, fuse_modules,
    MinMaxObserver, default_observer, default_weight_observer
)
from torch.ao.quantization.backend_config import (
    BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType
)
from torch.ao.quantization.observer import default_per_channel_weight_observer
from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx, prepare_qat_fx
from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)
from torch.utils.data import DataLoader
from torch._export import capture_pre_autograd_graph

# Local application imports (if any)

# Configuration
@hydra.main(config_path="conf", config_name="config")
def load_config(cfg: DictConfig) -> Dict[str, Any]:
    return cfg

# Centralized random seed setting
def set_random_seed(seed: int):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

class ModelStorageManagement:
    @staticmethod
    def save_quantized_model(model: nn.Module, path: str) -> None:
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), path)
        logger.info(f"Saved quantized model to {path}")

    @staticmethod
    def load_quantized_model(model: nn.Module, path: str) -> nn.Module:
        if not Path(path).exists():
            raise FileNotFoundError(f"Model file not found: {path}")
        model.load_state_dict(torch.load(path))
        return model
        
    @staticmethod
    def save_scripted_quantized_model(model: nn.Module, path: str) -> None:
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        scripted_model = torch.jit.script(model)
        torch.jit.save(scripted_model, path)
        logger.info(f"Saved scripted quantized model to {path}")

    @staticmethod
    def load_scripted_quantized_model(path: str) -> torch.jit.ScriptModule:
        if not Path(path).exists():
            raise FileNotFoundError(f"Model file not found: {path}")
        return torch.jit.load(path)

    @staticmethod
    def export_torchscript(model: nn.Module, example_inputs: torch.Tensor, path: str):
        """
        Export the quantized model to TorchScript format.
        """
        model.eval()
        traced_model = torch.jit.trace(model, example_inputs)
        torch.jit.save(traced_model, path)
        logger.info(f"Exported TorchScript model to {path}")

    @staticmethod
    def convert_to_torchscript(model: nn.Module, example_inputs: torch.Tensor) -> torch.jit.ScriptModule:
        """
        Convert the quantized model to TorchScript format for mobile deployment.
        """
        model.eval()
        scripted_model = torch.jit.trace(model, example_inputs)
        return torch.jit.optimize_for_inference(scripted_model)

    @staticmethod
    def export_onnx(model: nn.Module, example_inputs: torch.Tensor, path: str):
        """
        Export the quantized model to ONNX format.
        """
        model.eval()
        torch.onnx.export(model, example_inputs, path, opset_version=13)
        logger.info(f"Exported ONNX model to {path}")

    @staticmethod
    def export_quantized_model(model: nn.Module, example_input: torch.Tensor, export_path: str, export_format: str = 'torchscript'):
        """
        Export the quantized model to various formats for deployment.
        """
        model.eval()

        if export_format == 'torchscript':
            scripted_model = torch.jit.trace(model, example_input)
            torch.jit.save(scripted_model, export_path)
        elif export_format == 'onnx':
            torch.onnx.export(model, example_input, export_path, opset_version=13)
        else:
            raise ValueError(f"Unsupported export format: {export_format}")

        logger.info(f"Exported quantized model to {export_path} in {export_format} format")

class QuantizationConfig(BaseModel):
    backend: str = Field("x86", description="Quantization backend")
    use_fx_graph_mode: bool = Field(True, description="Use FX graph mode for quantization")
    use_dynamic_quantization: bool = Field(False, description="Use dynamic quantization")
    use_static_quantization: bool = Field(True, description="Use static quantization")
    use_qat: bool = Field(False, description="Use Quantization Aware Training")
    skip_symbolic_trace_modules: List[str] = Field(default_factory=list, description="Modules to skip during symbolic tracing")
    prepare_custom_config: Dict[str, Any] = Field(default_factory=dict, description="Custom configuration for prepare step")
    convert_custom_config: Dict[str, Any] = Field(default_factory=dict, description="Custom configuration for convert step")
    log_file: str = Field("quantization.log", description="Log file path")
    use_custom_module_handling: bool = Field(False, description="Use custom module handling")
    use_enhanced_benchmarking: bool = Field(False, description="Use enhanced benchmarking")

class QuantizationBackendBase(ABC):
    @abstractmethod
    def get_default_qconfig(self) -> QConfig:
        pass

    @abstractmethod
    def get_default_qat_qconfig(self) -> QConfig:
        pass

    @abstractmethod
    def get_qconfig_mapping(self) -> QConfigMapping:
        pass

    @abstractmethod
    def create_backend_quantizer(self) -> Optional[Any]:
        pass

class X86QuantizationBackend(QuantizationBackendBase):
    def get_default_qconfig(self) -> QConfig:
        return get_default_qconfig('x86')

    def get_default_qat_qconfig(self) -> QConfig:
        return get_default_qat_qconfig('x86')

    def get_qconfig_mapping(self) -> QConfigMapping:
        return get_default_qconfig_mapping('x86')

    def create_backend_quantizer(self) -> Optional[Any]:
        return None

class XNNPACKQuantizationBackend(QuantizationBackendBase):
    def get_default_qconfig(self) -> QConfig:
        return get_default_qconfig('xnnpack')

    def get_default_qat_qconfig(self) -> QConfig:
        return get_default_qat_qconfig('xnnpack')

    def get_qconfig_mapping(self) -> QConfigMapping:
        return QConfigMapping().set_global(self.get_default_qconfig())

    def create_backend_quantizer(self) -> Optional[XNNPACKQuantizer]:
        quantizer = XNNPACKQuantizer()
        quantizer.set_global(get_symmetric_quantization_config())
        return quantizer

# Factory for creating quantization backends
class QuantizationBackendFactory:
    @staticmethod
    def create_backend(backend: str) -> QuantizationBackendBase:
        if backend == 'x86':
            return X86QuantizationBackend()
        elif backend == 'xnnpack':
            return XNNPACKQuantizationBackend()
        else:
            raise ValueError(f"Unsupported backend: {backend}")

###
class QuantizationBackend:
    def __init__(self, backend: str):
        self.backend = backend
        torch.backends.quantized.engine = self.backend

    def get_default_qconfig(self) -> QConfig:
        return get_default_qconfig(self.backend)

    def get_default_qat_qconfig(self) -> QConfig:
        return get_default_qat_qconfig(self.backend)

    def get_qconfig_mapping(self) -> QConfigMapping:
        if self.backend == 'onednn':
            return get_default_qconfig_mapping('onednn')
        else:
            return QConfigMapping().set_global(self.get_default_qconfig())

    def create_backend_quantizer(self) -> Optional[XNNPACKQuantizer]:
        if self.backend == 'xnnpack':
            quantizer = XNNPACKQuantizer()
            quantizer.set_global(get_symmetric_quantization_config())
            return quantizer
        return None
    
    def create_backend_pattern_config(self, pattern: str, observation_type: str, dtype_config: Dict[str, torch.dtype]) -> BackendPatternConfig:
        return BackendPatternConfig(pattern) \
            .set_observation_type(observation_type) \
            .add_dtype_config(dtype_config)

    def setup_fusion(self, pattern: str, fused_module: nn.Module, fuser_method: callable) -> BackendPatternConfig:
        return BackendPatternConfig(pattern) \
            .set_fused_module(fused_module) \
            .set_fuser_method(fuser_method)

    def set_custom_qconfig(self, qconfig: QConfig) -> None:
        self.qconfig = qconfig
        self.qconfig_mapping = QConfigMapping().set_global(self.qconfig)
    
    def set_backend_config(self, backend_config: BackendConfig) -> None:
        self.backend_config = backend_config

    def set_backend(self, backend: str) -> None:
        supported_backends = ['x86', 'qnnpack', 'onednn', 'xnnpack']
        if backend not in supported_backends:
            raise ValueError(f"Supported backends are {', '.join(supported_backends)}")
        self.backend = backend
        torch.backends.quantized.engine = self.backend
    
    def set_qconfig_mapping(self, qconfig_mapping: QConfigMapping) -> None:
        self.qconfig_mapping = qconfig_mapping

    def auto_select_qconfig(self, model: nn.Module, example_inputs: torch.Tensor) -> QConfigMapping:
        qconfig_mapping = QConfigMapping()
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                if module.in_features < 256:
                    qconfig_mapping.set_module_name(name, default_per_channel_qconfig)
                else:
                    qconfig_mapping.set_module_name(name, default_qconfig)
        return qconfig_mapping

class QuantizationMetrics:
    @staticmethod
    def compare_accuracy(float_model: nn.Module, quant_model: nn.Module, 
                         test_data: torch.Tensor, target_data: torch.Tensor,
                         metric_fn: Callable[[torch.Tensor, torch.Tensor], float]) -> Tuple[float, float]:
        float_model.eval()
        quant_model.eval()
        
        with torch.no_grad():
            float_output = float_model(test_data)
            quant_output = quant_model(test_data)
        
        float_accuracy = metric_fn(float_output, target_data)
        quant_accuracy = metric_fn(quant_output, target_data)
        
        return float_accuracy, quant_accuracy

    @staticmethod
    def evaluate_accuracy(model: nn.Module, input_data: torch.Tensor,
                          target_data: torch.Tensor, criterion: nn.Module) -> float:
        model.eval()
        with torch.no_grad():
            output = model(input_data)
            loss: torch.Tensor = criterion(output, target_data)
            _, predicted = torch.max(output, 1)
            accuracy = (predicted == target_data).float().mean().item()
        return accuracy

class FXQuantization:
    @staticmethod
    def prepare_fx(
        model: nn.Module, 
        qconfig_mapping: QConfigMapping, 
        example_inputs: torch.Tensor, 
        prepare_custom_config_dict: Dict[str, Any], 
        backend_config: Optional[BackendConfig] = None
    ) -> nn.Module:
        return prepare_fx(
            model, 
            qconfig_mapping, 
            example_inputs,
            prepare_custom_config_dict=prepare_custom_config_dict,
            backend_config=backend_config
        )

    @staticmethod
    def prepare_qat_fx(
        model: nn.Module, 
        qconfig_mapping: QConfigMapping, 
        example_inputs: torch.Tensor,
        prepare_custom_config_dict: Dict[str, Any], 
        backend_config: Optional[BackendConfig] = None
    ) -> nn.Module:
        return prepare_qat_fx(
            model, 
            qconfig_mapping, 
            example_inputs,
            prepare_custom_config_dict=prepare_custom_config_dict,
            backend_config=backend_config
        )

    @staticmethod
    def convert_fx(
        prepared_model: nn.Module, 
        convert_custom_config_dict: Dict[str, Any],
        backend_config: Optional[BackendConfig] = None
    ) -> nn.Module:
        return convert_fx(
            prepared_model,
            convert_custom_config_dict=convert_custom_config_dict,
            backend_config=backend_config
        )

    @staticmethod
    def quantize_per_channel(model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        qconfig_mapping = QConfigMapping().set_global(default_per_channel_qconfig)
        prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs)
        quantized_model = quantize_fx.convert_fx(prepared_model)
        return quantized_model

    @staticmethod
    def quantize_dynamic(model: nn.Module, example_inputs: torch.Tensor, use_fx_graph_mode: bool) -> nn.Module:
        qconfig_mapping = QConfigMapping().set_global(default_dynamic_qconfig)
        if use_fx_graph_mode:
            prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs)
            quantized_model = quantize_fx.convert_fx(prepared_model)
        else:
            quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec=qconfig_mapping)
        return quantized_model

class QuantizerBase(ABC):
    @abstractmethod
    def prepare_model(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        pass

    @abstractmethod
    def quantize_model(self, model: nn.Module) -> nn.Module:
        pass

class StaticQuantizer(QuantizerBase):
    def __init__(self, qconfig: QConfig):
        self.qconfig = qconfig

    def prepare_model(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        model.eval()
        model.qconfig = self.qconfig
        return prepare(model)

    def quantize_model(self, prepared_model: nn.Module) -> nn.Module:
        return convert(prepared_model)

    def calibrate_model(self, prepared_model: nn.Module, calibration_data: torch.Tensor, num_batches: int = 100):
        prepared_model.eval()
        with torch.no_grad():
            for i, data in enumerate(calibration_data):
                if i >= num_batches:
                    break
                prepared_model(data)
                if i % 10 == 0:
                    logger.info(f"Calibration progress: {i}/{num_batches}")

class DynamicQuantizer(QuantizerBase):
    def __init__(self, qconfig_spec: Dict[Any, Any]):
        self.qconfig_spec = qconfig_spec

    def prepare_model(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        return model  # Dynamic quantization doesn't require preparation

    def quantize_model(self, prepared_model: nn.Module) -> nn.Module:
        return torch.quantization.quantize_dynamic(prepared_model, qconfig_spec=self.qconfig_spec)
      
class QATQuantizer(QuantizerBase):
    def __init__(self, qconfig: QConfig):
        self.qconfig = qconfig

    def prepare_model(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        model.train()
        model.qconfig = self.qconfig
        return prepare_qat(model)

    def quantize_model(self, prepared_model: nn.Module) -> nn.Module:
        return convert(prepared_model)

    def set_qat_learning_rate(self, optimizer: torch.optim.Optimizer, lr: float):
        """
        Set the learning rate for Quantization-Aware Training.
        """
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

    def quantization_aware_training(self, model: nn.Module,
                                    train_loader: DataLoader,
                                    optimizer: torch.optim.Optimizer,
                                    criterion: nn.Module,
                                    num_epochs: int,
                                    device: torch.device) -> nn.Module:
        prepared_model = self.prepare_model(model, next(iter(train_loader))[0]).to(device)
        
        for epoch in range(num_epochs):
            for inputs, targets in train_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                optimizer.zero_grad()
                outputs = prepared_model(inputs)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
            
            logger.info(f"QAT Epoch {epoch+1}/{num_epochs} completed")
        
        return self.quantize_model(prepared_model)

    def apply_quantization_aware_training(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        num_epochs: int,
        device: torch.device
    ) -> nn.Module:
        """
        Apply Quantization-Aware Training (QAT) to the model.
        """
        model.train()
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        model_prepared = torch.quantization.prepare_qat(model)

        for epoch in range(num_epochs):
            for inputs, targets in train_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                optimizer.zero_grad()
                outputs = model_prepared(inputs)
                loss: torch.Tensor = criterion(outputs, targets)
                loss.backward()
                optimizer.step()

            logger.info(f"QAT Epoch {epoch + 1}/{num_epochs} completed")

        model_quantized = torch.quantization.convert(model_prepared.eval(), inplace=False)
        return model_quantized
    
    def apply_knowledge_distillation(
        self, 
        student_model: nn.Module, 
        teacher_model: nn.Module,
        alpha: float = 0.5, 
        temperature: float = 2.0
    ) -> Callable:
        """
        Apply knowledge distillation during quantization-aware training.
        """
        def distillation_loss(student_outputs, teacher_outputs, targets, criterion):
            hard_loss = criterion(student_outputs, targets)
            soft_loss = nn.KLDivLoss()(F.log_softmax(student_outputs / temperature, dim=1),
                                       F.softmax(teacher_outputs / temperature, dim=1))
            return (1 - alpha) * hard_loss + alpha * soft_loss * (temperature ** 2)
        
        return distillation_loss

# Factory for creating quantizers
class QuantizerFactory:
    @staticmethod
    def create_quantizer(config: QuantizationConfig, backend: QuantizationBackendBase) -> QuantizerBase:
        if config.use_dynamic_quantization:
            return DynamicQuantizer({nn.Linear: torch.quantization.default_dynamic_qconfig})
        elif config.use_qat:
            return QATQuantizer(backend.get_default_qat_qconfig())
        else:
            return StaticQuantizer(backend.get_default_qconfig())

class QuantizationAnalyzer:
    def __init__(self) -> None:
        pass
    
    @staticmethod
    def get_memory_footprint(model: nn.Module) -> float:
        mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])
        mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])
        mem_total = mem_params + mem_bufs
        return mem_total / (1024 * 1024)  # Convert to MB
    
    @staticmethod
    def get_model_size(model: nn.Module) -> float:
        buffer = io.BytesIO()
        torch.save(model.state_dict(), buffer)
        size = buffer.getbuffer().nbytes / 1e6  # Size in MB
        return size

    @staticmethod
    def compare_models(model1: nn.Module, model2: nn.Module) -> Dict[str, Any]:
        comparison = {
            'param_count1': sum(p.numel() for p in model1.parameters()),
            'param_count2': sum(p.numel() for p in model2.parameters()),
            'memory_footprint1': QuantizationAnalyzer.get_memory_footprint(model1),
            'memory_footprint2': QuantizationAnalyzer.get_memory_footprint(model2),
        }
        
        comparison['param_count_diff'] = comparison['param_count1'] - comparison['param_count2']
        comparison['memory_footprint_diff'] = comparison['memory_footprint1'] - comparison['memory_footprint2']
        comparison['memory_reduction_percent'] = (1 - comparison['memory_footprint2'] / comparison['memory_footprint1']) * 100
        
        return comparison

    @staticmethod
    def benchmark_model(model: nn.Module, input_data: torch.Tensor, num_runs: int = 100, device: Optional[torch.device] = None) -> Dict[str, float]:
        model.eval()
        if device is None:
            device = next(model.parameters()).device
        input_data = input_data.to(device)
        model = model.to(device)

        # Warmup
        for _ in range(10):
            _ = model(input_data)

        # Benchmark
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(input_data)
        end_time = time.time()

        avg_time = (end_time - start_time) / num_runs
        throughput = 1 / avg_time  # inferences per second

        return {
            'avg_inference_time_ms': avg_time * 1000,
            'throughput_fps': throughput,
        }

    @staticmethod
    def analyze_quantization(float_model: nn.Module, quant_model: nn.Module,
                             example_inputs: torch.Tensor) -> Dict[str, Any]:
        if isinstance(float_model, torch.jit.ScriptModule):
            float_model = capture_pre_autograd_graph(float_model, example_inputs)
        if isinstance(quant_model, torch.jit.ScriptModule):
            quant_model = capture_pre_autograd_graph(quant_model, example_inputs)
        
        analysis = {}
        for (name, float_module), (_, quant_module) in zip(float_model.named_modules(), quant_model.named_modules()):
            if isinstance(quant_module, torch.ao.quantization.QuantizedModule):
                analysis[name] = {
                    'weight_range': (float_module.weight.min().item(), float_module.weight.max().item()),
                    'activation_range': (quant_module.activation_post_process.min_val.item(), 
                                         quant_module.activation_post_process.max_val.item()),
                    'weight_scale': quant_module.weight_scale,
                    'weight_zero_point': quant_module.weight_zero_point,
                }
            elif hasattr(quant_module, 'weight_fake_quant'):
                analysis[name] = {
                    'weight_range': (float_module.weight.min().item(), float_module.weight.max().item()),
                    'weight_scale': quant_module.weight_fake_quant.scale,
                    'weight_zero_point': quant_module.weight_fake_quant.zero_point,
                }
        return analysis

    @staticmethod
    def visualize_quantization(float_model: nn.Module, quant_model: nn.Module, 
                               example_inputs: Optional[torch.Tensor] = None, 
                               plot_type: str = 'both', 
                               comparison_type: str = 'both'):
        def plot_comparison(float_data, quant_data, diff_data, title_prefix):
            plt.figure(figsize=(15, 5))
            
            if plot_type in ['distribution', 'both']:
                plt.subplot(131)
                plt.hist(float_data.flatten(), bins=50, alpha=0.5, label='Float')
                plt.hist(quant_data.flatten(), bins=50, alpha=0.5, label='Quant')
                plt.legend()
                plt.title(f'{title_prefix} Distribution')
                plt.xlabel('Values')
                plt.ylabel('Frequency')
            
            if plot_type in ['difference', 'both']:
                plt.subplot(132)
                plt.hist(diff_data.flatten(), bins=50)
                plt.title(f'{title_prefix} Difference')
                plt.xlabel('Difference')
                plt.ylabel('Frequency')
            
            if plot_type in ['scatter', 'both']:
                plt.subplot(133)
                plt.scatter(float_data.flatten(), quant_data.flatten(), alpha=0.1)
                plt.plot([float_data.min(), float_data.max()], 
                        [float_data.min(), float_data.max()], 'r--')
                plt.xlabel(f'Float {title_prefix}')
                plt.ylabel(f'Quant {title_prefix}')
                plt.title(f'Float vs Quant {title_prefix}')
            
            plt.tight_layout()
            plt.show()

        if comparison_type in ['weights', 'both']:
            for (name, float_module), (_, quant_module) in zip(float_model.named_modules(), quant_model.named_modules()):
                if isinstance(quant_module, torch.ao.quantization.QuantizedModule):
                    float_weight = float_module.weight.detach().cpu().numpy()
                    quant_weight = quant_module.weight().dequantize().detach().cpu().numpy()
                    diff_weight = np.abs(float_weight - quant_weight)
                    
                    logger.info(f"Module: {name}")
                    logger.info(f"Max absolute difference: {diff_weight.max()}")
                    logger.info(f"Mean absolute difference: {diff_weight.mean()}")
                    
                    plot_comparison(float_weight, quant_weight, diff_weight, f'Weights ({name})')

        if comparison_type in ['outputs', 'both']:
            if example_inputs is None:
                raise ValueError("example_inputs must be provided for output comparison")
            
            float_model.eval()
            quant_model.eval()
            
            with torch.no_grad():
                float_output = float_model(example_inputs).cpu().numpy()
                quant_output = quant_model(example_inputs).cpu().numpy()
            
            diff_output = np.abs(float_output - quant_output)
            
            logger.info(f"Output - Max absolute difference: {diff_output.max()}")
            logger.info(f"Output - Mean absolute difference: {diff_output.mean()}")
            
            plot_comparison(float_output, quant_output, diff_output, 'Outputs')
    
    @staticmethod
    def profile_model(model: nn.Module, input_data: torch.Tensor, num_runs: int = 100):
        """
        Profile the model to identify performance bottlenecks.
        """
        model.eval()
        device = next(model.parameters()).device
        input_data = input_data.to(device)

        with torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            for _ in range(num_runs):
                model(input_data)

        logger.info(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
        return prof

class QuantizationManager:
    def __init__(self, cfg: Union[QuantizationConfig, DictConfig]):
        self.cfg = cfg if isinstance(cfg, QuantizationConfig) else QuantizationConfig(**cfg)
        
        self.backend = QuantizationBackendFactory.create_backend(self.cfg.backend)
        self.quantizer = QuantizerFactory.create_quantizer(self.cfg, self.backend)
        
        self.fx_quantization = FXQuantization()
        self.metrics = QuantizationMetrics()
        self.analyzer = QuantizationAnalyzer()
        self.model_storage = ModelStorageManagement()
        
        self.qconfig = self.backend.get_default_qconfig()
        self.qat_qconfig = self.backend.get_default_qat_qconfig()
        self.qconfig_mapping = self.backend.get_qconfig_mapping()

        self.use_pt2e = False
        
        logger.add(cfg.log_file, rotation="500 MB")

    def prepare_model(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        if self.cfg.use_fx_graph_mode:
            return self._prepare_fx(model, example_inputs)
        elif self.use_pt2e:
            return self._prepare_pt2e(model, example_inputs, self.cfg.use_qat)
        else:
            return self._prepare_eager(model)
    
    def _prepare_fx(self, model: nn.Module, example_inputs: torch.Tensor) -> nn.Module:
        if self.cfg.use_qat:
            return self.fx_quantization.prepare_qat_fx(model, self.qconfig_mapping, example_inputs,
                prepare_custom_config_dict=self.cfg.prepare_custom_config,
                backend_config=getattr(self.backend, 'backend_config', None))
        else:
            return self.fx_quantization.prepare_fx(model, self.qconfig_mapping, example_inputs,
                prepare_custom_config_dict=self.cfg.prepare_custom_config,
                backend_config=getattr(self.backend, 'backend_config', None))

    def _prepare_eager(self, model: nn.Module) -> nn.Module:
        model.qconfig = self.qat_qconfig if self.cfg.use_qat else self.qconfig
        propagate_qconfig_(model)
        model = fuse_modules(model, self._get_fusable_modules(model))
        return prepare_qat(model) if self.cfg.use_qat else prepare(model)
    
    def _prepare_pt2e(self, model: nn.Module, example_inputs: torch.Tensor, is_qat: bool) -> nn.Module:
        exported_model = capture_pre_autograd_graph(model, example_inputs)
        prepared_model = prepare_pt2e(exported_model, self.quantizer)
        return prepared_model


    def quantize_model(self, prepared_model: nn.Module) -> nn.Module:
        logger.info("Starting model quantization")
        
        if self.cfg.use_fx_graph_mode:
            return self.fx_quantization.convert_fx(prepared_model, 
                                                   convert_custom_config_dict=self.cfg.convert_custom_config)
        elif self.use_pt2e:
            return convert_pt2e(prepared_model)
        else:
            return self.quantizer.quantize_model(prepared_model)

    def handle_non_traceable_module(self, module: nn.Module, config: Dict[str, Any]) -> nn.Module:
        logger.info(f"Custom handling for non-traceable module: {type(module).__name__}")
        # Implementation depends on the specific non-traceable module
        # This is a placeholder for custom handling logic
        return module

    def _get_fusable_modules(self, model: nn.Module) -> List[List[str]]:
        fusable_modules = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear)):
                module_sequence = [name]
                if hasattr(module, 'bias') and module.bias is not None:
                    module_sequence.append(name + '.bias')
                next_module = list(module.children())[0] if list(module.children()) else None
                if isinstance(next_module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                    module_sequence.append(name.rsplit('.', 1)[0] + '.' + list(module.named_children())[0][0])
                if isinstance(next_module, (nn.ReLU, nn.ReLU6)):
                    module_sequence.append(name.rsplit('.', 1)[0] + '.' + list(module.named_children())[0][0])
                if len(module_sequence) > 1:
                    fusable_modules.append(module_sequence)
        return fusable_modules

    def set_pt2e_quantization(self, enable: bool = True) -> None:
        self.use_pt2e = enable
        if enable:
            self.quantizer = self.backend.create_backend_quantizer()

    def fuse_model(self, model: nn.Module) -> nn.Module:
        model.eval()
        model = torch.quantization.fuse_modules(model, self._get_fusable_modules(model))
        return model

    def _get_observed_module(self, module: nn.Module, qconfig: QConfig) -> nn.Module:
        if isinstance(module, nn.Conv2d):
            return torch.ao.quantization.QuantizedConv2d.from_float(module)
        elif isinstance(module, nn.Linear):
            return torch.ao.quantization.QuantizedLinear.from_float(module)
        else:
            raise ValueError(f"Unsupported module type: {type(module)}")

    @torch.jit.script
    def optimize_for_inference(self, model: nn.Module) -> nn.Module:
        """
        Optimize the quantized model for inference.
        """
        model.eval()
        if self.use_fx_graph_mode:
            model = convert_fx(model)
        else:
            model = torch.quantization.convert(model)
        return torch.jit.optimize_for_inference(torch.jit.script(model))

    def quantize_embedding(
        self, 
        embedding: nn.Embedding, 
        num_bits: int = 8
    ) -> nn.Embedding:
        """
        Quantize an embedding layer.
        """
        embedding.weight.data = torch.quantize_per_tensor(embedding.weight.data, 1 / 2**(num_bits-1), 0, torch.qint8)
        return embedding

    def apply_cross_layer_equalization(self, model: nn.Module) -> nn.Module:
        """
        Apply Cross-Layer Equalization (CLE) to improve quantization accuracy.
        """
        def equalize_weights(conv1, bn1, conv2):
            var_eps = 1e-5
            bn_std = torch.sqrt(bn1.running_var + var_eps)
            scale = (bn1.weight / bn_std).reshape(-1, 1, 1, 1)
            
            conv1.weight.data *= scale
            if conv1.bias is not None:
                conv1.bias.data *= bn1.weight / bn_std
            
            conv2.weight.data *= (1 / scale).reshape(1, -1, 1, 1)
            
            bn1.running_mean.data.zero_()
            bn1.running_var.data.fill_(1.0)
            bn1.weight.data.fill_(1.0)
            bn1.bias.data.zero_()
            
        for name, module in model.named_children():
            if isinstance(module, nn.Sequential):
                for i in range(len(module) - 2):
                    if isinstance(module[i], nn.Conv2d) and \
                       isinstance(module[i+1], nn.BatchNorm2d) and \
                       isinstance(module[i+2], nn.Conv2d):
                        equalize_weights(module[i], module[i+1], module[i+2])
            
            self.apply_cross_layer_equalization(module)
        
        return model

    def apply_bias_correction(self, model: nn.Module) -> nn.Module:
        """
        Apply bias correction to compensate for quantization errors.
        """
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)) and module.bias is not None:
                module.bias.data += 0.5 * module.weight.data.mean(dim=0)
        return model

    @staticmethod
    def set_random_seed(seed: int):
        """
        Set random seed for reproducibility.
        """
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
    
    def create_lr_scheduler(self, optimizer: torch.optim.Optimizer, num_epochs: int) -> torch.optim.lr_scheduler._LRScheduler:
        """
        Create a learning rate scheduler for quantization-aware training.
        """
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    def apply_pruning(self, model: nn.Module, amount: float = 0.5) -> nn.Module:
        """
        Apply pruning to the model before quantization.
        """
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                prune.l1_unstructured(module, name='weight', amount=amount)
                prune.remove(module, 'weight')
        return model

    def quantize_custom_module(self, module: nn.Module, 
                               quantization_config: Dict[str, Any]) -> nn.Module:
        class QuantizedCustomModule(nn.Module):
            def __init__(self, orig_module: nn.Module, qconfig):
                super().__init__()
                self.orig_module = orig_module
                self.qconfig = qconfig
                self.weight_fake_quant = qconfig.weight()
                self.activation_post_process = qconfig.activation()

            def forward(self, x):
                weight_quant = self.weight_fake_quant(self.orig_module.weight)
                out = self.orig_module._conv_forward(x, weight_quant, self.orig_module.bias)
                return self.activation_post_process(out)

        qconfig = QConfig(
            activation=quantization_config.get('activation', default_observer),
            weight=quantization_config.get('weight', default_weight_observer)
        )
        return QuantizedCustomModule(module, qconfig)
