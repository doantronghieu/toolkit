# My code starts from here
import copy
import io
import time
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, Any, Optional, Tuple, List, Callable
import torch
import torchvision
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.ao.quantization
from torch.ao.quantization.backend_config import (
    BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType
)
import torch.ao.quantization.quantize_fx as quantize_fx
from torch.ao.quantization import (
    QConfigMapping, float16_dynamic_qconfig, default_per_channel_qconfig, 
    default_qconfig, QConfig, get_default_qconfig, get_default_qat_qconfig, 
    get_default_qconfig_mapping, propagate_qconfig_, default_dynamic_qconfig, 
    prepare_qat, prepare, convert, fuse_modules,
    MinMaxObserver, default_observer, default_weight_observer
)
from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx, prepare_qat_fx
from torch.ao.quantization.observer import default_per_channel_weight_observer
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
  XNNPACKQuantizer,
  get_symmetric_quantization_config,
)

class QuantizationManager:
    def __init__(self, backend: str = 'x86'):
        self.backend = backend
        self.backend_config = None
        self.use_backend_config = False
        self.backend_quantizer = None
        self.use_pt2e = False
        self.use_xnnpack = False
        self.xnnpack_quantizer = None
        
        self.qconfig = get_default_qconfig(self.backend)
        self.qat_qconfig = get_default_qat_qconfig(self.backend)
        torch.backends.quantized.engine = self.backend

        if self.backend == 'onednn':
            self.qconfig_mapping = get_default_qconfig_mapping('onednn')
        else:
            self.qconfig_mapping = QConfigMapping().set_global(self.qconfig)

        # New attributes for FX-specific configurations
        self.skip_symbolic_trace_modules = []
        self.prepare_custom_config_dict = {}
        self.convert_custom_config_dict = {}
        
        # Feature flags
        self.use_fx_graph_mode = False
        self.use_dynamic_quantization = False
        self.use_static_quantization = False
        self.use_qat = False
        
        self._use_custom_module_handling = False
        self._use_enhanced_benchmarking = False
        
        self.quantizer = None

    def use_pretrained_quantized_model(self, model_name: str) -> nn.Module:
        """
        Use a pretrained quantized model from torchvision.
        """
        if not hasattr(torchvision.models.quantization, model_name):
            raise ValueError(f"Quantized model {model_name} not available in torchvision")
        
        return getattr(torchvision.models.quantization, model_name)(pretrained=True, quantize=True)
    
    # Feature flag getters and setters
    @property
    def set_use_backend_config(self, enable: bool):
        self.use_backend_config = enable

    @property
    def set_use_pt2e(self, enable: bool):
        self.use_pt2e = enable
        if enable:
            self.xnnpack_quantizer = self._create_xnnpack_quantizer()

    @property
    def use_fx_graph_mode(self):
        return self.use_fx_graph_mode

    @use_fx_graph_mode.setter
    def use_fx_graph_mode(self, value: bool):
        self.use_fx_graph_mode = value

    @property
    def use_dynamic_quantization(self):
        return self.use_dynamic_quantization

    @use_dynamic_quantization.setter
    def use_dynamic_quantization(self, value: bool):
        self.use_dynamic_quantization = value

    @property
    def use_custom_module_handling(self):
        return self._use_custom_module_handling

    @use_custom_module_handling.setter
    def use_custom_module_handling(self, value: bool):
        self._use_custom_module_handling = value

    @property
    def use_enhanced_benchmarking(self):
        return self._use_enhanced_benchmarking

    @use_enhanced_benchmarking.setter
    def use_enhanced_benchmarking(self, value: bool):
        self._use_enhanced_benchmarking = value
    
    def set_backend_config(self, backend_config: BackendConfig):
        """
        Set a custom BackendConfig for quantization.
        """
        self.backend_config = backend_config

    def set_backend(self, backend: str):
        """
        Set the quantization backend.
        """
        if backend not in ['x86', 'qnnpack']:
            raise ValueError("Supported backends are 'x86' and 'qnnpack'")
        self.backend = backend
        torch.backends.quantized.engine = self.backend
        self.qconfig = get_default_qconfig(self.backend)
        self.qconfig_mapping = QConfigMapping().set_global(self.qconfig)
    
    def create_dtype_config(self, input_dtype, output_dtype, weight_dtype, bias_dtype):
        """
        Create a DTypeConfig with the specified dtypes.
        """
        return DTypeConfig(
            input_dtype=input_dtype,
            output_dtype=output_dtype,
            weight_dtype=weight_dtype,
            bias_dtype=bias_dtype
        )

    def create_backend_pattern_config(self, pattern, observation_type, dtype_config):
        """
        Create a BackendPatternConfig for a specific pattern.
        """
        return BackendPatternConfig(pattern) \
            .set_observation_type(observation_type) \
            .add_dtype_config(dtype_config)

    def setup_fusion(self, pattern, fused_module, fuser_method):
        """
        Set up fusion for a specific pattern.
        """
        return BackendPatternConfig(pattern) \
            .set_fused_module(fused_module) \
            .set_fuser_method(fuser_method)
    
    def prepare_model(
        self,
        model: torch.nn.Module,
        example_inputs: Optional[torch.Tensor] = None,
        is_qat: bool = False,
        is_dynamic: bool = False, 
        quantizable_ops: Optional[List[torch.nn.Module]] = None
    ) -> torch.nn.Module:
        """
        Prepare the model for quantization.
        
        Args:
            model: The model to be prepared for quantization.
            example_inputs: Example inputs for the model (required for FX graph mode).
            is_qat: Whether to prepare for quantization-aware training.
            is_dynamic: Whether to prepare for dynamic quantization.
            quantizable_ops: List of quantizable operations (for eager mode).
        
        Returns:
            Prepared model ready for calibration or quantization.
        """
        model = copy.deepcopy(model)
        
        if self.use_dynamic_quantization or is_dynamic:
            return self._prepare_dynamic(model, quantizable_ops)
        
        if self.use_pt2e:
            exported_model = capture_pre_autograd_graph(model, example_inputs)
            return prepare_pt2e(exported_model, self.backend_quantizer)
        
        if self.use_fx_graph_mode:
            return self._prepare_fx(model, example_inputs, is_qat)
        else:
            return self._prepare_eager(model, is_qat, quantizable_ops)

    def _prepare_fx(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor, 
        is_qat: bool
    ) -> torch.nn.Module:
        if not self._is_traceable(model):
            if self.use_custom_module_handling:
                return self.handle_non_traceable_module(model, self.prepare_custom_config_dict)
            else:
                raise ValueError("Model is not symbolically traceable. Enable custom module handling or check the model architecture.")

        self.prepare_custom_config_dict["non_traceable_module_name"] = self.skip_symbolic_trace_modules

        if is_qat:
            prepared_model = prepare_qat_fx(model, self.qconfig_mapping, example_inputs,
                prepare_custom_config_dict=self.prepare_custom_config_dict,
                backend_config=self.backend_config)
        else:
            prepared_model = prepare_fx(model, self.qconfig_mapping, example_inputs,
                prepare_custom_config_dict=self.prepare_custom_config_dict,
                backend_config=self.backend_config)
        return prepared_model

    def _prepare_eager(self, model: torch.nn.Module, is_qat: bool, quantizable_ops: Optional[List[torch.nn.Module]]) -> torch.nn.Module:
        model.eval() if not is_qat else model.train()
        model.qconfig = self.qat_qconfig if is_qat else self.qconfig
        
        if quantizable_ops:
            propagate_qconfig_(model, qconfig_dict={op: self.qconfig for op in quantizable_ops})
        else:
            propagate_qconfig_(model)
        
        model = fuse_modules(model, self._get_fusable_modules(model))
        
        return prepare_qat(model) if is_qat else prepare(model)

    def _prepare_dynamic(self, model: torch.nn.Module, quantizable_ops: Optional[List[torch.nn.Module]] = None) -> torch.nn.Module:
        if quantizable_ops:
            qconfig_dict = {op: default_dynamic_qconfig for op in quantizable_ops}
        else:
            qconfig_dict = {
                torch.nn.Linear: default_dynamic_qconfig,
                torch.nn.LSTM: default_dynamic_qconfig,
                torch.nn.GRU: default_dynamic_qconfig,
                torch.nn.RNN: default_dynamic_qconfig,
            }
        
        model.qconfig = QConfig(
            activation=MinMaxObserver.with_args(dtype=torch.qint8),
            weight=default_per_channel_weight_observer.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
        )
        
        return prepare(model, qconfig_dict=qconfig_dict)

    def _is_traceable(self, model: torch.nn.Module) -> bool:
        """
        Check if the model is symbolically traceable.
        """
        try:
            torch.jit.script(model)
            return True
        except Exception:
            return False

    def annotate_model(self, model: torch.nn.Module, annotations: dict):
        for name, module in model.named_modules():
            if name in annotations:
                module.quantization_annotation = annotations[name]
    
    def quantize_model(
        self, 
        prepared_model: torch.nn.Module,
        is_dynamic: bool = False,
        is_per_channel: bool = False,
        example_inputs = None,
    ) -> torch.nn.Module:
        """
        Convert the prepared model to a quantized model.
        
        Args:
            prepared_model: The prepared model ready for quantization.
            is_dynamic: Whether the model was prepared for dynamic quantization.
        
        Returns:
            Quantized model.
        """
        if is_dynamic:
            return convert(prepared_model)
        
        if self.use_pt2e:
            exported_model = capture_pre_autograd_graph(prepared_model, example_inputs)
            prepared_model = prepare_pt2e(exported_model, self.quantizer)
            # Calibration should be performed here
            quantized_model = convert_pt2e(prepared_model)
            return quantized_model
        
        if self.use_fx_graph_mode:
            if is_per_channel:
                self.qconfig_mapping = QConfigMapping().set_global(default_per_channel_qconfig)
            quantized_model = convert_fx(prepared_model, 
                                         convert_custom_config_dict=self.convert_custom_config_dict,
                                         backend_config=self.backend_config)
        else:
            quantized_model = convert(prepared_model)
        return quantized_model

    def handle_non_traceable_module(self, module: torch.nn.Module, config: Dict[str, Any]) -> torch.nn.Module:
        """
        Handle non-traceable modules by applying custom quantization techniques.
        """
        # Implementation depends on the specific non-traceable module
        # This is a placeholder for custom handling logic
        print(f"Custom handling for non-traceable module: {type(module).__name__}")
        return module

    def _get_fusable_modules(self, model: torch.nn.Module) -> List[List[str]]:
        fusable_modules = []
        for name, module in model.named_modules():
            if isinstance(module, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d, torch.nn.Linear)):
                module_sequence = [name]
                if hasattr(module, 'bias') and module.bias is not None:
                    module_sequence.append(name + '.bias')
                next_module = list(module.children())[0] if list(module.children()) else None
                if isinstance(next_module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):
                    module_sequence.append(name.rsplit('.', 1)[0] + '.' + list(module.named_children())[0][0])
                if isinstance(next_module, (torch.nn.ReLU, torch.nn.ReLU6)):
                    module_sequence.append(name.rsplit('.', 1)[0] + '.' + list(module.named_children())[0][0])
                if len(module_sequence) > 1:
                    fusable_modules.append(module_sequence)
        return fusable_modules

    def set_pt2e_quantization(self, enable: bool = True):
        self.use_pt2e = enable
        if enable:
            self.quantizer = self.create_backend_quantizer()

    def create_backend_quantizer(self):
        if self.backend == 'xnnpack':
            quantizer = XNNPACKQuantizer()
            quantizer.set_global(get_symmetric_quantization_config())
            return quantizer
        # Add more backend quantizers as needed
        return None

    def _prepare_pt2e(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor, 
        is_qat: bool
    ):
        exported_model = capture_pre_autograd_graph(model, example_inputs)
        prepared_model = prepare_pt2e(exported_model, self.quantizer)
        return prepared_model

    def analyze_quantization(
        self, 
        float_model: torch.nn.Module, 
        quant_model: torch.nn.Module,
        example_inputs: torch.Tensor, 
    ) -> Dict[str, Any]:
        """
        Analyze the quantization results.
        
        Args:
            float_model: The original floating-point model.
            quant_model: The quantized model.
        
        Returns:
            Dictionary containing quantization analysis results.
        """
        if self.use_pt2e:
            float_model = capture_pre_autograd_graph(float_model, example_inputs)
            quant_model = capture_pre_autograd_graph(quant_model, example_inputs)
        
        analysis = {}
        for (name, float_module), (_, quant_module) in zip(float_model.named_modules(), quant_model.named_modules()):
            if isinstance(quant_module, torch.ao.quantization.QuantizedModule):
                analysis[name] = {
                    'weight_range': (float_module.weight.min().item(), float_module.weight.max().item()),
                    'activation_range': (quant_module.activation_post_process.min_val.item(), 
                                         quant_module.activation_post_process.max_val.item()),
                    'weight_scale': quant_module.weight_scale,
                    'weight_zero_point': quant_module.weight_zero_point,
                }
            elif hasattr(quant_module, 'weight_fake_quant'):
                analysis[name] = {
                    'weight_range': (float_module.weight.min().item(), float_module.weight.max().item()),
                    'weight_scale': quant_module.weight_fake_quant.scale,
                    'weight_zero_point': quant_module.weight_fake_quant.zero_point,
                }
        return analysis

    def auto_select_qconfig(self, model: torch.nn.Module, example_inputs: torch.Tensor) -> QConfigMapping:
        qconfig_mapping = QConfigMapping()
        for name, module in model.named_modules():
            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):
                if module.in_features < 256:
                    qconfig_mapping.set_module_name(name, default_per_channel_qconfig)
                else:
                    qconfig_mapping.set_module_name(name, default_qconfig)
        return qconfig_mapping

    def benchmark_model(
        self, 
        model: torch.nn.Module,
        input_data: torch.Tensor, 
        target_data: torch.Tensor,
        num_runs: int = 100,
        criterion: Optional[torch.nn.Module] = None
    ) -> Dict[str, float]:
        if self.use_pt2e:
            model = capture_pre_autograd_graph(model, input_data)
        
        model.eval()
        torch.cuda.empty_cache()
        
        with torch.no_grad():
            # Warmup
            for _ in range(10):
                _ = model(input_data)
            
            # Benchmark
            start_time = time.time()
            for _ in range(num_runs):
                output = model(input_data)
            end_time = time.time()
            
        avg_time = (end_time - start_time) / num_runs
        throughput = 1 / avg_time  # inferences per second
        model_size = self.get_model_size(model)
        
        result = {
            'avg_inference_time': avg_time * 1000,  # ms
            'throughput': throughput,
            'model_size_mb': model_size,
            'inferences_per_mb': throughput / model_size,
        }

        if criterion is not None and target_data is not None:
            accuracy = self.evaluate_accuracy(model, input_data, target_data, criterion)
            result['accuracy'] = accuracy

        if self.use_enhanced_benchmarking:
            result.update(self._enhanced_benchmark_metrics(model, input_data))

        return result

    def evaluate_accuracy(
        self, 
        model: torch.nn.Module, 
        input_data: torch.Tensor,
        target_data: torch.Tensor, 
        criterion: torch.nn.Module
    ) -> float:
        model.eval()
        with torch.no_grad():
            output = model(input_data)
            loss = criterion(output, target_data)
            _, predicted = torch.max(output, 1)
            accuracy = (predicted == target_data).float().mean().item()
        return accuracy

    def _enhanced_benchmark_metrics(self, model: torch.nn.Module, input_data: torch.Tensor) -> Dict[str, float]:
        memory_usage = torch.cuda.max_memory_allocated() / 1e6 if torch.cuda.is_available() else 0
        return {
            'peak_memory_usage_mb': memory_usage,
            'parameter_count': sum(p.numel() for p in model.parameters()),
        }
    
    def calibrate_model(
        self,
        prepared_model: torch.nn.Module,
        calibration_data: torch.Tensor,
        num_batches: int = 100
    ) -> None:
        """
        Calibrate the prepared model using the provided calibration data.
        
        Args:
            prepared_model: The prepared model ready for calibration.
            calibration_data: Tensor containing calibration data.
            num_batches: Number of batches to use for calibration.
        """
        prepared_model.eval()
        with torch.no_grad():
            for i, data in enumerate(calibration_data):
                if i >= num_batches:
                    break
                prepared_model(data)
                if i % 10 == 0:
                    print(f"Calibration progress: {i}/{num_batches}")

    def save_quantized_model(self, model: torch.nn.Module, path: str) -> None:
        """
        Save the quantized model.
        """
        if self.use_pt2e:
            torch.export.save(model, path)
        else:
          torch.save(model.state_dict(), path)

    def load_quantized_model(self, model: torch.nn.Module, path: str) -> torch.nn.Module:
        """
        Load a quantized model.
        """
        if self.use_pt2e:
            return torch.export.load(path)
        else:
          model.load_state_dict(torch.load(path))
          return model

    def save_scripted_quantized_model(self, model: torch.nn.Module, path: str) -> None:
        """
        Save the quantized model as a TorchScript model.
        """
        scripted_model = torch.jit.script(model)
        torch.jit.save(scripted_model, path)

    def load_scripted_quantized_model(self, path: str) -> torch.jit.ScriptModule:
        """
        Load a TorchScript quantized model.
        """
        return torch.jit.load(path)

    @staticmethod
    def get_model_size(model: torch.nn.Module) -> float:
        """
        Get the size of the model in MB.
        
        Args:
            model: The model to measure.
        
        Returns:
            Size of the model in MB.
        """
        buffer = io.BytesIO()
        torch.save(model.state_dict(), buffer)
        size = buffer.getbuffer().nbytes / 1e6  # Size in MB
        return size

    def visualize_weight_comparison(self, float_model: torch.nn.Module, quant_model: torch.nn.Module):
        for (name, float_module), (_, quant_module) in zip(float_model.named_modules(), quant_model.named_modules()):
            if isinstance(quant_module, torch.ao.quantization.QuantizedModule):
                float_weight = float_module.weight.detach().numpy()
                quant_weight = quant_module.weight().dequantize().detach().numpy()
                
                print(f"Module: {name}")
                print(f"Max absolute difference: {np.abs(float_weight - quant_weight).max()}")
                print(f"Mean absolute difference: {np.abs(float_weight - quant_weight).mean()}")
                
                plt.figure(figsize=(12, 4))
                plt.subplot(131)
                plt.hist(float_weight.flatten(), bins=50, alpha=0.5, label='Float')
                plt.hist(quant_weight.flatten(), bins=50, alpha=0.5, label='Quant')
                plt.legend()
                plt.title('Weight Distribution')
                
                plt.subplot(132)
                plt.hist((float_weight - quant_weight).flatten(), bins=50)
                plt.title('Weight Difference')
                
                plt.subplot(133)
                plt.scatter(float_weight.flatten(), quant_weight.flatten(), alpha=0.1)
                plt.plot([-1, 1], [-1, 1], 'r--')
                plt.xlabel('Float Weights')
                plt.ylabel('Quant Weights')
                plt.title('Float vs Quant Weights')
                
                plt.tight_layout()
                plt.show()
    
    def compare_accuracy(self, float_model: torch.nn.Module, quant_model: torch.nn.Module, 
                         test_data: torch.Tensor, target_data: torch.Tensor,
                         metric_fn: Callable[[torch.Tensor, torch.Tensor], float]) -> Tuple[float, float]:
        """
        Compare the accuracy of float and quantized models.
        """
        float_model.eval()
        quant_model.eval()
        
        with torch.no_grad():
            float_output = float_model(test_data)
            quant_output = quant_model(test_data)
        
        float_accuracy = metric_fn(float_output, target_data)
        quant_accuracy = metric_fn(quant_output, target_data)
        
        return float_accuracy, quant_accuracy

    def quantization_aware_training(
        self,
        model: torch.nn.Module,
        train_loader,
        optimizer: torch.optim.Optimizer,
        criterion,
        num_epochs
    ):
        prepared_model = self.prepare_model(model, next(iter(train_loader))[0], is_qat=True)
        
        for epoch in range(num_epochs):
            for inputs, targets in train_loader:
                optimizer.zero_grad()
                outputs = prepared_model(inputs)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
        
        return self.quantize_model(prepared_model)

    def set_custom_qconfig(self, qconfig: QConfig) -> None:
        """
        Set a custom quantization configuration.
        """
        self.qconfig = qconfig
        self.qconfig_mapping = QConfigMapping().set_global(self.qconfig)

    def apply_mixed_precision_quantization(self, model: torch.nn.Module, 
                                           example_inputs: torch.Tensor) -> torch.nn.Module:
        """
        Apply mixed precision quantization to the model.
        """
        qconfig_mapping = QConfigMapping()
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                if module.out_features > 1000:
                    qconfig_mapping.set_module_name(name, float16_dynamic_qconfig)
                else:
                    qconfig_mapping.set_module_name(name, default_qconfig)
            elif isinstance(module, torch.nn.Conv2d):
                qconfig_mapping.set_module_name(name, default_qconfig)

        prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs)
        quantized_model = quantize_fx.convert_fx(prepared_model)
        return quantized_model

    def apply_post_training_dynamic_quantization(
        self, 
        model: nn.Module, 
        qconfig_spec: Dict[Any, Any] = None
    ) -> nn.Module:
        """
        Apply post-training dynamic quantization to the model.
        """
        self.use_dynamic_quantization = True
        if qconfig_spec is None:
            qconfig_spec = {nn.Linear, nn.LSTM}
        
        return torch.quantization.quantize_dynamic(model, qconfig_spec=qconfig_spec, dtype=torch.qint8)

    def apply_post_training_static_quantization(
        self, 
        model: nn.Module, 
        example_inputs: torch.Tensor
    ) -> nn.Module:
        """
        Apply post-training static quantization to the model.
        """
        self.use_static_quantization = True
        model.eval()
        model.qconfig = self.qconfig
        torch.quantization.prepare(model, inplace=True)
        model(example_inputs)  # Calibration
        torch.quantization.convert(model, inplace=True)
        return model

    def apply_quantization_aware_training(
        self, 
        model: nn.Module, 
        example_inputs: torch.Tensor
    ) -> nn.Module:
        """
        Apply quantization-aware training to the model.
        """
        self.use_qat = True
        model.train()
        model.qconfig = self.qat_qconfig
        torch.quantization.prepare_qat(model, inplace=True)
        # QAT training loop should be implemented separately
        torch.quantization.convert(model, inplace=True)
        return model

    def auto_select_quantization_approach(
        self, 
        model: nn.Module, 
        example_inputs: torch.Tensor
    ) -> nn.Module:
        """
        Automatically select and apply the most appropriate quantization approach.
        """
        if self.backend == 'qnnpack':
            return self.apply_post_training_static_quantization(model, example_inputs)
        elif any(isinstance(m, (nn.LSTM, nn.GRU)) for m in model.modules()):
            return self.apply_post_training_dynamic_quantization(model)
        else:
            return self.apply_quantization_aware_training(model, example_inputs)

    def quantize_custom_module(self, module: torch.nn.Module, 
                               quantization_config: Dict[str, Any]) -> torch.nn.Module:
        """
        Quantize a custom module using the provided configuration.
        """
        class QuantizedCustomModule(torch.nn.Module):
            def __init__(self, orig_module: nn.Module, qconfig):
                super().__init__()
                self.orig_module = orig_module
                self.qconfig = qconfig
                self.weight_fake_quant = qconfig.weight()
                self.activation_post_process = qconfig.activation()

            def forward(self, x):
                weight_quant = self.weight_fake_quant(self.orig_module.weight)
                out = self.orig_module._conv_forward(x, weight_quant, self.orig_module.bias)
                return self.activation_post_process(out)

        qconfig = QConfig(
            activation=quantization_config.get('activation', default_observer),
            weight=quantization_config.get('weight', default_weight_observer)
        )
        return QuantizedCustomModule(module, qconfig)

    def set_skip_symbolic_trace_modules(self, module_list: List[str]):
        """
        Set the list of modules to skip during symbolic tracing.
        """
        self.skip_symbolic_trace_modules = module_list

    def set_prepare_custom_config(self, config: Dict[str, Any]):
        """
        Set custom configuration for the prepare step in FX Graph Mode Quantization.
        """
        self.prepare_custom_config_dict = config

    def set_convert_custom_config(self, config: Dict[str, Any]):
        """
        Set custom configuration for the convert step in FX Graph Mode Quantization.
        """
        self.convert_custom_config_dict = config

    def get_qconfig_mapping(self) -> QConfigMapping:
        """
        Get the current QConfigMapping.
        """
        return self.qconfig_mapping

    def set_qconfig_mapping(self, qconfig_mapping: QConfigMapping):
        """
        Set a custom QConfigMapping.
        """
        self.qconfig_mapping = qconfig_mapping

    def fuse_model(
        self, 
        model: torch.nn.Module
    ) -> torch.nn.Module:
        """
        Fuse modules in the model for improved performance.
        """
        model.eval()
        model = torch.quantization.fuse_modules(model, self._get_fusable_modules(model))
        return model

    def _get_observed_module(
        self, 
        module: torch.nn.Module, 
        qconfig: QConfig
    ) -> torch.nn.Module:
        """
        Get the observed version of a module for a given QConfig.
        """
        if isinstance(module, torch.nn.Conv2d):
            return torch.ao.quantization.QuantizedConv2d.from_float(module)
        elif isinstance(module, torch.nn.Linear):
            return torch.ao.quantization.QuantizedLinear.from_float(module)
        else:
            raise ValueError(f"Unsupported module type: {type(module)}")

    def optimize_for_inference(
        self, 
        model: torch.nn.Module
    ) -> torch.nn.Module:
        """
        Optimize the quantized model for inference.
        """
        model.eval()
        if self.use_fx_graph_mode:
            model = convert_fx(model)
        else:
            model = torch.quantization.convert(model)
        return torch.jit.script(model)

    def quantize_per_channel(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor
    ) -> torch.nn.Module:
        """
        Apply per-channel quantization to the model.
        """
        qconfig_mapping = QConfigMapping().set_global(default_per_channel_qconfig)
        prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs)
        quantized_model = quantize_fx.convert_fx(prepared_model)
        return quantized_model

    def quantize_dynamic(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor
    ) -> torch.nn.Module:
        """
        Apply dynamic quantization to the model.
        """
        qconfig_mapping = QConfigMapping().set_global(default_dynamic_qconfig)
        if self.use_fx_graph_mode:
            prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs)
            quantized_model = quantize_fx.convert_fx(prepared_model)
        else:
            quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec=qconfig_mapping)
        return quantized_model

    def export_torchscript(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor, 
        path: str
    ):
        """
        Export the quantized model to TorchScript format.
        """
        model.eval()
        traced_model = torch.jit.trace(model, example_inputs)
        torch.jit.save(traced_model, path)

    def convert_to_torchscript(
        self, 
        model: nn.Module, 
        example_inputs: torch.Tensor
    ) -> torch.jit.ScriptModule:
        """
        Convert the quantized model to TorchScript format for mobile deployment.
        """
        model.eval()
        scripted_model = torch.jit.trace(model, example_inputs)
        return torch.jit.optimize_for_inference(scripted_model)

    def export_onnx(
        self, 
        model: torch.nn.Module, 
        example_inputs: torch.Tensor, 
        path: str
    ):
        """
        Export the quantized model to ONNX format.
        """
        model.eval()
        torch.onnx.export(model, example_inputs, path, opset_version=11)

    def set_qat_learning_rate(
        self, 
        optimizer: torch.optim.Optimizer, 
        lr: float
    ):
        """
        Set the learning rate for Quantization-Aware Training.
        """
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

    def quantize_embedding(
        self, 
        embedding: torch.nn.Embedding, 
        num_bits: int = 8
    ) -> torch.nn.Embedding:
        """
        Quantize an embedding layer.
        """
        embedding.weight.data = torch.quantize_per_tensor(embedding.weight.data, 1 / 2**(num_bits-1), 0, torch.qint8)
        return embedding

    def apply_cross_layer_equalization(
        self, 
        model: torch.nn.Module
    ) -> torch.nn.Module:
        """
        Apply Cross-Layer Equalization (CLE) to improve quantization accuracy.
        """
        # This is a placeholder implementation. CLE requires a more complex implementation
        # that analyzes and adjusts weights across multiple layers.
        return model

    def apply_bias_correction(
        self, 
        model: torch.nn.Module
    ) -> torch.nn.Module:
        """
        Apply bias correction to compensate for quantization errors.
        """
        for name, module in model.named_modules():
            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)) and module.bias is not None:
                # This is a simplified bias correction. A more accurate implementation
                # would involve analyzing the quantization error and adjusting accordingly.
                module.bias.data += 0.5 * module.weight.data.mean(dim=0)
        return model

    def visualize_quantization_effects(
        self, 
        float_model: torch.nn.Module, 
        quant_model: torch.nn.Module,
        example_inputs: torch.Tensor
    ):
        float_model.eval()
        quant_model.eval()
        
        with torch.no_grad():
            float_output = float_model(example_inputs)
            quant_output = quant_model(example_inputs)
        
        diff = (float_output - quant_output).abs()
        
        print(f"Max absolute difference: {diff.max().item()}")
        print(f"Mean absolute difference: {diff.mean().item()}")
        
        plt.figure(figsize=(12, 4))
        plt.subplot(131)
        plt.hist(float_output.flatten().numpy(), bins=50, alpha=0.5, label='Float')
        plt.hist(quant_output.flatten().numpy(), bins=50, alpha=0.5, label='Quant')
        plt.legend()
        plt.title('Output Distribution')
        
        plt.subplot(132)
        plt.hist(diff.flatten().numpy(), bins=50)
        plt.title('Output Difference')
        
        plt.subplot(133)
        plt.scatter(float_output.flatten().numpy(), quant_output.flatten().numpy(), alpha=0.1)
        plt.plot([-1, 1], [-1, 1], 'r--')
        plt.xlabel('Float Outputs')
        plt.ylabel('Quant Outputs')
        plt.title('Float vs Quant Outputs')
        
        plt.tight_layout()
        plt.show()

    def get_memory_footprint(self, model: torch.nn.Module) -> float:
        """
        Get the memory footprint of the model in MB.
        """
        mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])
        mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])
        mem_total = mem_params + mem_bufs
        return mem_total / (1024 * 1024)  # Convert to MB

    def compare_models(self, model1: torch.nn.Module, model2: torch.nn.Module) -> Dict[str, Any]:
        """
        Compare two models (e.g., float vs quantized) and return various metrics.
        """
        comparison = {
            'param_count1': sum(p.numel() for p in model1.parameters()),
            'param_count2': sum(p.numel() for p in model2.parameters()),
            'memory_footprint1': self.get_memory_footprint(model1),
            'memory_footprint2': self.get_memory_footprint(model2),
        }
        
        comparison['param_count_diff'] = comparison['param_count1'] - comparison['param_count2']
        comparison['memory_footprint_diff'] = comparison['memory_footprint1'] - comparison['memory_footprint2']
        comparison['memory_reduction_percent'] = (1 - comparison['memory_footprint2'] / comparison['memory_footprint1']) * 100
        
        return comparison
